{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1AJcklGwS60uIldQRnWamCrd5zv0BaImg",
     "timestamp": 1702525935264
    }
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 0. 라이브러리 가져오기 및 하이퍼파라미터 설정"
   ],
   "metadata": {
    "id": "r01zdMqIb8Jt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 라이브러리 불러오기"
   ],
   "metadata": {
    "id": "o-fTRZJBcGPR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xI8bR6HRUNg8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "MuviGqwjWmyw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 하이퍼파라미터 설정"
   ],
   "metadata": {
    "id": "xxqqUO2-cDT6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_ratio = 0.9\n",
    "num_class = 2\n",
    "length=33\n",
    "\n",
    "batch_size = 16\n",
    "GRU_hidden_size = 10\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100"
   ],
   "metadata": {
    "id": "ySZSqVtVWeYK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 데이터 불러오기"
   ],
   "metadata": {
    "id": "CrsdVQc1cJeY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mypath=\"./training_data/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ],
   "metadata": {
    "id": "93mLa4CqVJIH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_dict={}\n",
    "for name in onlyfiles:\n",
    "  data_dict[name[:-5]]=pd.read_excel(mypath+name)[\"current\"].to_numpy()"
   ],
   "metadata": {
    "id": "J7D_UaXamo7c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 데이터 전처리"
   ],
   "metadata": {
    "id": "gCJ4_l2CcPYa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for key, value in data_dict.items():\n",
    "  x_normalized = (value-min(value))/(max(value)-min(value))\n",
    "  data_dict[key]=x_normalized"
   ],
   "metadata": {
    "id": "Kf-DD2vrWGKg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def data_divide(data_dict, length=33):\n",
    "  for key,value in data_dict.items():\n",
    "    nd_array=np.zeros((value.shape[0]-length + 1, length), dtype=\"float\")\n",
    "    for k in range(nd_array.shape[0]):\n",
    "      nd_array[k]=value[k:k+length]\n",
    "    data_dict[key]=nd_array\n",
    "  return data_dict"
   ],
   "metadata": {
    "id": "A2bQLmanupg8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_dict=data_divide(data_dict, length=length)"
   ],
   "metadata": {
    "id": "wUcP_xt5xRO_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "num_of_data = list(data_dict.values())[0].shape[0]*len(data_dict)\n",
    "x_data=np.zeros((num_of_data,length),dtype=\"float\")\n",
    "y_data=np.zeros((num_of_data),dtype=\"int\")\n",
    "i=0\n",
    "for key,value in data_dict.items():\n",
    "  for k in range(value.shape[0]):\n",
    "    x_data[i]=value[k]\n",
    "    if key in [\"LG gram - LG gram (1차, 35퍼)\",\"LG gram - LG gram (2차, 35퍼)\",\"LG gram - LG gram (3차, 35퍼)\",\n",
    "               \"삼성 25w 충전기 - 갤럭시 s22+ (1차, 37퍼)\",\"삼성 25w 충전기 - 갤럭시 s22+ (2차, 37퍼)\",\"삼성 25w 충전기 - 갤럭시 s22+ (3차, 37퍼)\",\n",
    "               \"삼성 45w 충전기 - LG gram (1차, 32퍼)\",\"삼성 45w 충전기 - LG gram (2차, 32퍼)\",\"삼성 45w 충전기 - LG gram (3차, 32퍼)\",\n",
    "               \"삼성 45w 충전기 - 갤럭시 s22+ (1차, 27퍼)\",\"삼성 45w 충전기 - 갤럭시 s22+ (2차, 27퍼)\",\"삼성 45w 충전기 - 갤럭시 s22+ (3차, 27퍼)\",\n",
    "               \"삼성 45w 충전기 - 갤럭시 탭 s6 lite (1차, 6퍼)\",\"삼성 45w 충전기 - 갤럭시 탭 s6 lite (2차, 6퍼)\",\"삼성 45w 충전기 - 갤럭시 탭 s6 lite (3차, 6퍼)\",\n",
    "               ]:\n",
    "      y_data[i]=1\n",
    "    else:\n",
    "      y_data[i]=0\n",
    "    i+=1"
   ],
   "metadata": {
    "id": "HRSYmJRYy_7u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 훈련/테스트 데이터 분리"
   ],
   "metadata": {
    "id": "nD-21rX7cVhe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=train_ratio)"
   ],
   "metadata": {
    "id": "FGyoVk3GW7V5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 모델 구성"
   ],
   "metadata": {
    "id": "AaW2D2Jmccfu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GRU 모델"
   ],
   "metadata": {
    "id": "DL27OSracgF4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.name = \"GRU\"\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "id": "1KIJkBpdXXL4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. 모델용 데이터셋 준비"
   ],
   "metadata": {
    "id": "X_wSdRzSctP3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train_tensor = torch.Tensor(x_train).unsqueeze(1).to(DEVICE)\n",
    "x_test_tensor = torch.Tensor(x_test).unsqueeze(1).to(DEVICE)\n",
    "y_train_tensor = torch.Tensor(y_train).long().to(DEVICE)\n",
    "y_test_tensor = torch.Tensor(y_test).long().to(DEVICE)\n",
    "\n",
    "y_train_tensor = F.one_hot(y_train_tensor, num_classes=num_class)\n",
    "y_test_tensor = F.one_hot(y_test_tensor, num_classes=num_class)"
   ],
   "metadata": {
    "id": "oBR1uh8MW9aP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)"
   ],
   "metadata": {
    "id": "mePrcPKqXOFX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "id": "e-aDCLjZXWLk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. 모델 생성"
   ],
   "metadata": {
    "id": "zq642Z0zcwTn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# GPU .to(DEVICE) 설정 코드\n",
    "prediction_model = GRUModel(x_data.shape[1], GRU_hidden_size, 3, num_class).to(DEVICE)"
   ],
   "metadata": {
    "id": "m6tD_WM_X51R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. 모델 손실함수 및 Optimizer 설정"
   ],
   "metadata": {
    "id": "7IScJDFOcx_-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(prediction_model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "id": "Q139cqubX53m"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. 모델 훈련"
   ],
   "metadata": {
    "id": "25_doBJCc3OF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_loss_record = []\n",
    "test_loss_record = []"
   ],
   "metadata": {
    "id": "PIenHyiQfjCb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_and_test(model, train_loader, test_loader, optimizer, criterion, num_epochs, verbose=True):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.to(torch.float32), targets.to(torch.float32))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if verbose:\n",
    "          print(\"-----------------------------------------------------------------------------\")\n",
    "          print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "        train_loss_record.append(total_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.to(torch.float32), targets.to(torch.float32))\n",
    "                total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        if verbose:\n",
    "          print(f\"Test Loss: {avg_loss}\")\n",
    "          print(\"-----------------------------------------------------------------------------\")\n",
    "          print(\"\")\n",
    "        test_loss_record.append(total_loss / len(test_loader))"
   ],
   "metadata": {
    "id": "vQDzph3cX12j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Cuda : \", next(prediction_model.parameters()).is_cuda)\n",
    "train_and_test(prediction_model, train_loader, test_loader, optimizer, criterion, num_epochs)"
   ],
   "metadata": {
    "id": "2aNLj8mCYzX_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. 모델 평가 및 수정"
   ],
   "metadata": {
    "id": "b7Aac71visgI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "predict_y = np.argmax(prediction_model(x_test_tensor).cpu().detach().numpy(), axis = 1)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predict_y))\n",
    "print(\"AUROC Score: \", np.round(roc_auc_score(y_test, predict_y), 3))"
   ],
   "metadata": {
    "id": "rbmk0xCsbRk2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install optuna"
   ],
   "metadata": {
    "id": "030H6f_2_Tc5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import optuna"
   ],
   "metadata": {
    "id": "mvhQGd1q_hg-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "best_dict = {\"ROC\": 0.0}"
   ],
   "metadata": {
    "id": "coCz8HQCn8yE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        \"batch_size\" : trial.suggest_categorical('batch_size', [8, 16, 32, 64, 128]),\n",
    "        \"GRU_hidden_size\" : trial.suggest_int('GRU_hidden_size', 4, 20),\n",
    "        \"layer_dim\" : trial.suggest_int('layer_dim', 1, 5),\n",
    "        \"learning_rate\" : trial.suggest_float('learning_rate', 1e-4, 2e-3),\n",
    "        \"num_epochs\" : trial.suggest_int('num_epochs', 10, 60)\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = param[\"batch_size\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size = param[\"batch_size\"])\n",
    "\n",
    "    prediction_model = GRUModel(length, param[\"GRU_hidden_size\"], param[\"layer_dim\"], num_class).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(prediction_model.parameters(), lr=param[\"learning_rate\"])\n",
    "\n",
    "    train_and_test(prediction_model, train_loader, test_loader, optimizer, criterion, param[\"num_epochs\"], verbose=True)\n",
    "\n",
    "    predict_y = np.argmax(prediction_model(x_test_tensor).cpu().detach().numpy(), axis=1)\n",
    "    roc = np.round(roc_auc_score(y_test, predict_y), 3)\n",
    "\n",
    "    print(\"Final ROC:\", roc)\n",
    "\n",
    "    if roc > best_dict[\"ROC\"]:\n",
    "        best_dict[\"Model\"] = prediction_model\n",
    "        best_dict[\"ROC\"] = roc\n",
    "        best_dict[\"Hyperparam\"] = param\n",
    "        best_dict[\"train_loss\"] = train_loss_record[-param[\"num_epochs\"]:]\n",
    "        best_dict[\"test_loss\"] = test_loss_record[-param[\"num_epochs\"]:]\n",
    "        print(\"Best Model Updated\")\n",
    "\n",
    "    print(\"\")\n",
    "    return roc"
   ],
   "metadata": {
    "id": "0aF1KVhO_lWS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.enqueue_trial({\n",
    "    \"batch_size\": 8,\n",
    "    \"GRU_hidden_size\": 13,\n",
    "    \"layer_dim\": 4,\n",
    "    \"learning_rate\": 0.00035698599755058444,\n",
    "    \"num_epochs\": 45\n",
    "    })\n",
    "study.optimize(objective,n_trials=40)"
   ],
   "metadata": {
    "id": "EtB_4RHmDxsa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(best_dict)"
   ],
   "metadata": {
    "id": "X6KqRpVKpJlS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. 모델 저장 및 시각화"
   ],
   "metadata": {
    "id": "ljmfrNn7i5OZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(best_dict[\"train_loss\"], label=\"train\")\n",
    "plt.plot(best_dict[\"test_loss\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.title(\"Loss Results\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "wk4AUG5mee-9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "PATH = \"\"\n",
    "torch.save(prediction_model, PATH + \"_\"+ prediction_model.name + \"_Model.pt\")"
   ],
   "metadata": {
    "id": "qLZy07YrhdE0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open(PATH + prediction_model.name + \"_hyperparam.json\", \"w\") as json_file:\n",
    "    json.dump(best_dict[\"Hyperparam\"], json_file)"
   ],
   "metadata": {
    "id": "fD30HmnV661b"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
